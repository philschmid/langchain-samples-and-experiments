{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Summarize and extract information from Arxiv papers"]},{"cell_type":"markdown","metadata":{},"source":["install required dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install tokenizers langchain arxiv pypdf huggingface_hub easyllm --upgrade"]},{"cell_type":"markdown","metadata":{},"source":["read `.env` file for variables"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import os \n","os.environ = {**os.environ, **{env.split(\"=\")[0]: env.split(\"=\")[1] for env in open(\"../.env\", \"r\").readlines()}}"]},{"cell_type":"markdown","metadata":{},"source":["imports"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import arxiv\n","from langchain.document_loaders import PyPDFLoader\n","from tokenizers import Tokenizer\n","from langchain import HuggingFaceHub\n","from easyllm.prompt_utils import build_llama2_prompt\n"]},{"cell_type":"markdown","metadata":{},"source":["load arxiv paper as pdf and convert to text"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["title: Textbooks Are All You Need II: phi-1.5 technical report\n"]}],"source":["paper_id = \"2309.05463\"\n","\n","# get paper by id \n","paper = next(arxiv.Search(id_list=[paper_id]).results())\n","print(f\"title: {paper.title}\")\n","\n","# download paper\n","downloaded_file = paper.download_pdf(filename=f\"{paper_id}.pdf\")"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["loader = PyPDFLoader(downloaded_file)\n","pages = loader.load_and_split()\n","\n","# count tokens of the paper \n","# model_id = \"tiiuae/falcon-180B-chat\"\n","# max_total_tokens = 1500\n","model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n","max_total_tokens = 3000\n","tokenizer = Tokenizer.from_pretrained(model_id)\n","total_tokens = 0\n","paper_content = \"\"\n","\n","for page in pages:\n","    tokens_per_page = len(tokenizer.encode(page.page_content))\n","    total_tokens += tokens_per_page\n","    # add page content to paper content\n","    paper_content += page.page_content + \"\\n\"\n","    \n","    # check if prompt got too long\n","    if total_tokens > max_total_tokens:\n","        break"]},{"cell_type":"markdown","metadata":{},"source":["create prompt"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["prompt_template = \"\"\"please complete the following tasks for the <paper>. Your response should be a markdown document with one headings per task.\n","1. Extract the objective and contribution of the paper in one sentence. \n","2. Extract the implementation details as step-by-step instructions focus on technical details. \n","3. Extract the key insights and learnings of the paper as bullet points.\n","4. Extract the results of the paper in one sentence.\n","\n","<paper>{paper}</paper>\n","\"\"\"\n","\n","prompt = prompt_template.format(paper=paper_content)\n","prompt = build_llama2_prompt(prompt)"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["llm = HuggingFaceHub(repo_id=model_id, model_kwargs={\"temperature\": 0.01, \"max_new_tokens\": 1024, \"do_sample\": True})\n","\n","res = llm(prompt)"]},{"cell_type":"markdown","metadata":{},"source":["write result to file"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["with open(\"llama-paper.md\", \"w\") as f:\n","    f.write(res)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"hf","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
